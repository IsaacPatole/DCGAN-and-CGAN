{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cgan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9y1k2oOw8MrG",
        "colab_type": "code",
        "outputId": "d4053b2f-f688-48ad-f2b9-cafd522a159f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T8asaJVdVN6J",
        "colab_type": "code",
        "outputId": "03920c4a-12a0-484d-935e-4f6d5c401e9b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0-alpha0\n",
        "import tensorflow"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-alpha0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1a/66/32cffad095253219d53f6b6c2a436637bbe45ac4e7be0244557210dc3918/tensorflow_gpu-2.0.0a0-cp36-cp36m-manylinux1_x86_64.whl (332.1MB)\n",
            "\u001b[K     |████████████████████████████████| 332.1MB 66kB/s \n",
            "\u001b[?25hCollecting tb-nightly<1.14.0a20190302,>=1.14.0a20190301 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a9/51/aa1d756644bf4624c03844115e4ac4058eff77acd786b26315f051a4b195/tb_nightly-1.14.0a20190301-py3-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 25.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.16.4)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019030116,>=1.14.0.dev2019030115 (from tensorflow-gpu==2.0.0-alpha0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/13/82/f16063b4eed210dc2ab057930ac1da4fbe1e91b7b051a6c8370b401e6ae7/tf_estimator_nightly-1.14.0.dev2019030115-py2.py3-none-any.whl (411kB)\n",
            "\u001b[K     |████████████████████████████████| 419kB 43.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.7.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (3.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.1.7)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.33.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (0.8.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-alpha0) (1.15.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (0.15.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190302,>=1.14.0a20190301->tensorflow-gpu==2.0.0-alpha0) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-alpha0) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-alpha0) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow-gpu\n",
            "Successfully installed tb-nightly-1.14.0a20190301 tensorflow-gpu-2.0.0a0 tf-estimator-nightly-1.14.0.dev2019030115\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "foMzHI7_PKiL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "os.chdir(r\"/content/gdrive/My Drive/Colab Notebooks/GAN\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_88f6ThJOT9P",
        "colab_type": "code",
        "outputId": "cb7b582d-4a8b-462c-8585-b68ae1e37c25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11376
        }
      },
      "source": [
        "#Imorting the libraries\n",
        "from __future__ import print_function, division\n",
        "\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
        "from tensorflow.keras.layers import BatchNormalization, Activation, Embedding, ZeroPadding2D\n",
        "from tensorflow.keras.layers import LeakyReLU\n",
        "from tensorflow.keras.layers import UpSampling2D, Conv2D\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "#Creating a class.\n",
        "class CGAN():\n",
        "  #Lets Initilized all necessary variables\n",
        "    def __init__(self):\n",
        "        self.img_rows = 28\n",
        "        self.img_cols = 28\n",
        "        self.channels = 1\n",
        "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
        "        self.num_classes = 10\n",
        "        self.latent_dim = 100\n",
        "\n",
        "        optimizer = SGD(lr=0.1, momentum=0.5, decay=1.00004)\n",
        "\n",
        "        # Build the discriminator\n",
        "        self.discriminator = self.build_discriminator()\n",
        "        self.discriminator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer,\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "        # Build the generator\n",
        "        self.generator = self.build_generator()\n",
        "        self.generator.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "       \n",
        "        noise = Input(shape=(100,))\n",
        "        label = Input(shape=(1,))\n",
        "        img = self.generator([noise, label])\n",
        "\n",
        "        # For the combined model we will only train the generator\n",
        "        self.discriminator.trainable = False\n",
        "\n",
        "        # Pass generated image as input and determines validity to the descriminitor\n",
        "        # and the label of that image\n",
        "        valid = self.discriminator([img, label])\n",
        "\n",
        "        # The combined model  (stacked generator and discriminator) takes\n",
        "        # noise as input => generates images => determines validity\n",
        "        self.combined = Model([noise, label], valid)\n",
        "        self.combined.compile(loss=['binary_crossentropy'],\n",
        "            optimizer=optimizer)\n",
        "\n",
        "    def build_generator(self):\n",
        "        #Take the noise and label (0.9) \n",
        "        \n",
        "        noise = Input(shape=(self.latent_dim,))\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, self.latent_dim)(label))\n",
        "        model_input = multiply([noise, label_embedding])\n",
        "        \n",
        "        print(noise.shape)\n",
        "        print(label_embedding.shape)\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(200, input_dim=self.latent_dim))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "        model.add(Dense(1000))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "        model.add(Dense(1200, input_dim=self.latent_dim))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(BatchNormalization(momentum=0.8))\n",
        "\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Dense(np.prod(self.img_shape), activation='sigmoid'))\n",
        "        model.add(Reshape(self.img_shape))\n",
        "\n",
        "        model.summary()\n",
        "\n",
        "        img = model(model_input)\n",
        "        \n",
        "        return Model([noise, label], img)\n",
        "\n",
        "    def build_discriminator(self):\n",
        "        img = Input(shape=self.img_shape)\n",
        "        label = Input(shape=(1,), dtype='int32')\n",
        "        label_embedding = Flatten()(Embedding(self.num_classes, np.prod(self.img_shape))(label))\n",
        "        flat_img = Flatten()(img)\n",
        "        model_input = multiply([flat_img, label_embedding])\n",
        "\n",
        "        model = Sequential()\n",
        "\n",
        "        model.add(Dense(240,  input_dim=np.prod(self.img_shape)))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Dense(50))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Dense(240))\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dropout(0.5))\n",
        "\n",
        "        model.add(Dense(1, activation='sigmoid'))\n",
        "        model.summary()\n",
        "\n",
        "        validity = model(model_input)\n",
        "\n",
        "        return Model([img, label], validity)\n",
        "\n",
        "    def train(self, epochs, batch_size=100, save_interval=50):\n",
        "\n",
        "        # Load the dataset\n",
        "        (X_train, y_train), (_, _) = mnist.load_data()\n",
        "\n",
        "        # Rescale -1 to 1\n",
        "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
        "        X_train = np.expand_dims(X_train, axis=3)\n",
        "        y_train = y_train.reshape(-1, 1)\n",
        "\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Discriminator\n",
        "            # ---------------------\n",
        "\n",
        "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
        "            imgs, labels = X_train[idx], y_train[idx]\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "            gen_imgs = self.generator.predict([noise, labels])\n",
        "\n",
        "            valid = np.ones((batch_size, 1))\n",
        "            fake = np.zeros((batch_size, 1))\n",
        "\n",
        "            # Train the discriminator\n",
        "            d_loss_real = self.discriminator.train_on_batch([imgs, labels], valid)\n",
        "            \n",
        "            d_loss_fake = self.discriminator.train_on_batch([gen_imgs, labels], fake)\n",
        "            \n",
        "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
        "            \n",
        "\n",
        "            # ---------------------\n",
        "            #  Train Generator\n",
        "            # ---------------------\n",
        "\n",
        "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "\n",
        "            valid = np.ones((batch_size, 1))\n",
        "            # Generator wants discriminator to label the generated images as the intended\n",
        "            # digits\n",
        "            sampled_labels = np.random.randint(0, 10, batch_size).reshape(-1, 1)\n",
        "\n",
        "            # Train the generator\n",
        "            g_loss = self.combined.train_on_batch([noise, sampled_labels], valid)\n",
        "            # print(\"g loss\")\n",
        "            # print(g_loss)\n",
        "            # print(\"\")\n",
        "\n",
        "            # Plot the progress\n",
        "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
        "\n",
        "            # If at save interval => save generated image samples\n",
        "            if epoch % save_interval == 0:\n",
        "                self.save_imgs(epoch)\n",
        "\n",
        "    def save_imgs(self, epoch):\n",
        "        r, c = 2, 5\n",
        "        noise = np.random.normal(0, 1, (r * c, 100))\n",
        "        sampled_labels = np.arange(0, 10).reshape(-1, 1)\n",
        "\n",
        "        gen_imgs = self.generator.predict([noise, sampled_labels])\n",
        "\n",
        "        # Rescale images 0 - 1\n",
        "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
        "\n",
        "        fig, axs = plt.subplots(r, c)\n",
        "        fig.suptitle(\"CGAN: Generated digits\", fontsize=12)\n",
        "        cnt = 0\n",
        "        for i in range(r):\n",
        "            for j in range(c):\n",
        "                axs[i,j].imshow(gen_imgs[cnt,:,:,0], cmap='gray')\n",
        "                axs[i,j].set_title(\"Digit: %d\" % sampled_labels[cnt])\n",
        "                axs[i,j].axis('off')\n",
        "                cnt += 1\n",
        "        fig.savefig(\"images_%d.png\" % epoch)\n",
        "        plt.close()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    cgan = CGAN()\n",
        "    cgan.train(epochs=600, batch_size=32, save_interval=200)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 240)               188400    \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 240)               0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 240)               0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                12050     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 240)               12240     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 240)               0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 240)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 241       \n",
            "=================================================================\n",
            "Total params: 212,931\n",
            "Trainable params: 212,931\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "(None, 100)\n",
            "(None, 100)\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 200)               20200     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 200)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2 (Batc (None, 200)               800       \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1000)              201000    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 1000)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_1 (Ba (None, 1000)              4000      \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 1200)              1201200   \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_v2_2 (Ba (None, 1200)              4800      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 1200)              0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 784)               941584    \n",
            "_________________________________________________________________\n",
            "reshape (Reshape)            (None, 28, 28, 1)         0         \n",
            "=================================================================\n",
            "Total params: 2,373,584\n",
            "Trainable params: 2,368,784\n",
            "Non-trainable params: 4,800\n",
            "_________________________________________________________________\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0621 09:09:45.647927 140440944097152 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
            "W0621 09:09:46.877578 140440944097152 training.py:2131] Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0 [D loss: 0.718670, acc.: 7.81%] [G loss: 0.659844]\n",
            "1 [D loss: 0.698599, acc.: 48.44%] [G loss: 0.655864]\n",
            "2 [D loss: 0.697209, acc.: 48.44%] [G loss: 0.657297]\n",
            "3 [D loss: 0.696475, acc.: 50.00%] [G loss: 0.652890]\n",
            "4 [D loss: 0.696979, acc.: 50.00%] [G loss: 0.657061]\n",
            "5 [D loss: 0.694303, acc.: 50.00%] [G loss: 0.655903]\n",
            "6 [D loss: 0.694201, acc.: 50.00%] [G loss: 0.656350]\n",
            "7 [D loss: 0.692966, acc.: 50.00%] [G loss: 0.659948]\n",
            "8 [D loss: 0.693289, acc.: 50.00%] [G loss: 0.657782]\n",
            "9 [D loss: 0.693350, acc.: 48.44%] [G loss: 0.655864]\n",
            "10 [D loss: 0.693066, acc.: 50.00%] [G loss: 0.655937]\n",
            "11 [D loss: 0.694203, acc.: 50.00%] [G loss: 0.658318]\n",
            "12 [D loss: 0.693442, acc.: 50.00%] [G loss: 0.660178]\n",
            "13 [D loss: 0.693760, acc.: 50.00%] [G loss: 0.656997]\n",
            "14 [D loss: 0.695801, acc.: 50.00%] [G loss: 0.654951]\n",
            "15 [D loss: 0.691760, acc.: 50.00%] [G loss: 0.661331]\n",
            "16 [D loss: 0.694819, acc.: 50.00%] [G loss: 0.654748]\n",
            "17 [D loss: 0.693929, acc.: 48.44%] [G loss: 0.659709]\n",
            "18 [D loss: 0.694688, acc.: 50.00%] [G loss: 0.660108]\n",
            "19 [D loss: 0.696761, acc.: 50.00%] [G loss: 0.660140]\n",
            "20 [D loss: 0.694874, acc.: 50.00%] [G loss: 0.659180]\n",
            "21 [D loss: 0.693681, acc.: 50.00%] [G loss: 0.657834]\n",
            "22 [D loss: 0.693767, acc.: 46.88%] [G loss: 0.657563]\n",
            "23 [D loss: 0.691718, acc.: 50.00%] [G loss: 0.658832]\n",
            "24 [D loss: 0.693875, acc.: 50.00%] [G loss: 0.659582]\n",
            "25 [D loss: 0.693424, acc.: 50.00%] [G loss: 0.658265]\n",
            "26 [D loss: 0.693869, acc.: 50.00%] [G loss: 0.660430]\n",
            "27 [D loss: 0.694000, acc.: 50.00%] [G loss: 0.659082]\n",
            "28 [D loss: 0.695383, acc.: 48.44%] [G loss: 0.661551]\n",
            "29 [D loss: 0.693858, acc.: 46.88%] [G loss: 0.657585]\n",
            "30 [D loss: 0.690318, acc.: 50.00%] [G loss: 0.659918]\n",
            "31 [D loss: 0.695610, acc.: 50.00%] [G loss: 0.658250]\n",
            "32 [D loss: 0.693179, acc.: 50.00%] [G loss: 0.658546]\n",
            "33 [D loss: 0.695488, acc.: 48.44%] [G loss: 0.659573]\n",
            "34 [D loss: 0.693333, acc.: 50.00%] [G loss: 0.659786]\n",
            "35 [D loss: 0.692688, acc.: 51.56%] [G loss: 0.658297]\n",
            "36 [D loss: 0.693730, acc.: 48.44%] [G loss: 0.656961]\n",
            "37 [D loss: 0.692187, acc.: 51.56%] [G loss: 0.660155]\n",
            "38 [D loss: 0.691233, acc.: 50.00%] [G loss: 0.659640]\n",
            "39 [D loss: 0.692051, acc.: 50.00%] [G loss: 0.659309]\n",
            "40 [D loss: 0.693523, acc.: 50.00%] [G loss: 0.659941]\n",
            "41 [D loss: 0.695017, acc.: 48.44%] [G loss: 0.661206]\n",
            "42 [D loss: 0.695876, acc.: 48.44%] [G loss: 0.656696]\n",
            "43 [D loss: 0.694258, acc.: 50.00%] [G loss: 0.658735]\n",
            "44 [D loss: 0.693763, acc.: 50.00%] [G loss: 0.659751]\n",
            "45 [D loss: 0.693216, acc.: 50.00%] [G loss: 0.660360]\n",
            "46 [D loss: 0.694417, acc.: 48.44%] [G loss: 0.661103]\n",
            "47 [D loss: 0.696803, acc.: 50.00%] [G loss: 0.660036]\n",
            "48 [D loss: 0.692680, acc.: 51.56%] [G loss: 0.658291]\n",
            "49 [D loss: 0.692667, acc.: 48.44%] [G loss: 0.662935]\n",
            "50 [D loss: 0.695356, acc.: 50.00%] [G loss: 0.663017]\n",
            "51 [D loss: 0.690127, acc.: 50.00%] [G loss: 0.658743]\n",
            "52 [D loss: 0.692315, acc.: 46.88%] [G loss: 0.660267]\n",
            "53 [D loss: 0.691168, acc.: 50.00%] [G loss: 0.661124]\n",
            "54 [D loss: 0.691320, acc.: 50.00%] [G loss: 0.664333]\n",
            "55 [D loss: 0.691459, acc.: 46.88%] [G loss: 0.657212]\n",
            "56 [D loss: 0.692289, acc.: 50.00%] [G loss: 0.658973]\n",
            "57 [D loss: 0.693387, acc.: 50.00%] [G loss: 0.661132]\n",
            "58 [D loss: 0.692587, acc.: 50.00%] [G loss: 0.657133]\n",
            "59 [D loss: 0.692583, acc.: 50.00%] [G loss: 0.660948]\n",
            "60 [D loss: 0.692775, acc.: 50.00%] [G loss: 0.660602]\n",
            "61 [D loss: 0.694227, acc.: 50.00%] [G loss: 0.662384]\n",
            "62 [D loss: 0.692490, acc.: 50.00%] [G loss: 0.660930]\n",
            "63 [D loss: 0.694944, acc.: 46.88%] [G loss: 0.659414]\n",
            "64 [D loss: 0.696805, acc.: 50.00%] [G loss: 0.662258]\n",
            "65 [D loss: 0.693051, acc.: 50.00%] [G loss: 0.658379]\n",
            "66 [D loss: 0.693976, acc.: 48.44%] [G loss: 0.661286]\n",
            "67 [D loss: 0.691719, acc.: 50.00%] [G loss: 0.661244]\n",
            "68 [D loss: 0.694065, acc.: 50.00%] [G loss: 0.659958]\n",
            "69 [D loss: 0.694492, acc.: 50.00%] [G loss: 0.661602]\n",
            "70 [D loss: 0.695920, acc.: 50.00%] [G loss: 0.662598]\n",
            "71 [D loss: 0.696266, acc.: 50.00%] [G loss: 0.659331]\n",
            "72 [D loss: 0.691184, acc.: 50.00%] [G loss: 0.658705]\n",
            "73 [D loss: 0.692562, acc.: 48.44%] [G loss: 0.660600]\n",
            "74 [D loss: 0.694935, acc.: 50.00%] [G loss: 0.661268]\n",
            "75 [D loss: 0.695455, acc.: 50.00%] [G loss: 0.661613]\n",
            "76 [D loss: 0.694110, acc.: 50.00%] [G loss: 0.663159]\n",
            "77 [D loss: 0.694480, acc.: 48.44%] [G loss: 0.661203]\n",
            "78 [D loss: 0.692275, acc.: 50.00%] [G loss: 0.661475]\n",
            "79 [D loss: 0.689303, acc.: 50.00%] [G loss: 0.661526]\n",
            "80 [D loss: 0.693280, acc.: 50.00%] [G loss: 0.661779]\n",
            "81 [D loss: 0.694054, acc.: 50.00%] [G loss: 0.661210]\n",
            "82 [D loss: 0.692256, acc.: 50.00%] [G loss: 0.662336]\n",
            "83 [D loss: 0.693515, acc.: 50.00%] [G loss: 0.659776]\n",
            "84 [D loss: 0.692162, acc.: 48.44%] [G loss: 0.658462]\n",
            "85 [D loss: 0.695104, acc.: 48.44%] [G loss: 0.660940]\n",
            "86 [D loss: 0.693389, acc.: 46.88%] [G loss: 0.659764]\n",
            "87 [D loss: 0.692379, acc.: 50.00%] [G loss: 0.660916]\n",
            "88 [D loss: 0.692719, acc.: 50.00%] [G loss: 0.662548]\n",
            "89 [D loss: 0.691170, acc.: 50.00%] [G loss: 0.662415]\n",
            "90 [D loss: 0.690220, acc.: 50.00%] [G loss: 0.662656]\n",
            "91 [D loss: 0.693702, acc.: 50.00%] [G loss: 0.663051]\n",
            "92 [D loss: 0.691910, acc.: 50.00%] [G loss: 0.661290]\n",
            "93 [D loss: 0.694636, acc.: 48.44%] [G loss: 0.661089]\n",
            "94 [D loss: 0.689683, acc.: 50.00%] [G loss: 0.661068]\n",
            "95 [D loss: 0.696980, acc.: 45.31%] [G loss: 0.661854]\n",
            "96 [D loss: 0.693987, acc.: 50.00%] [G loss: 0.661222]\n",
            "97 [D loss: 0.694103, acc.: 50.00%] [G loss: 0.661864]\n",
            "98 [D loss: 0.694575, acc.: 48.44%] [G loss: 0.659083]\n",
            "99 [D loss: 0.693812, acc.: 48.44%] [G loss: 0.662034]\n",
            "100 [D loss: 0.697147, acc.: 46.88%] [G loss: 0.661119]\n",
            "101 [D loss: 0.694838, acc.: 46.88%] [G loss: 0.661717]\n",
            "102 [D loss: 0.693778, acc.: 50.00%] [G loss: 0.659134]\n",
            "103 [D loss: 0.694495, acc.: 48.44%] [G loss: 0.660914]\n",
            "104 [D loss: 0.690392, acc.: 50.00%] [G loss: 0.662013]\n",
            "105 [D loss: 0.694276, acc.: 46.88%] [G loss: 0.662156]\n",
            "106 [D loss: 0.693319, acc.: 50.00%] [G loss: 0.662951]\n",
            "107 [D loss: 0.692092, acc.: 50.00%] [G loss: 0.659857]\n",
            "108 [D loss: 0.695030, acc.: 48.44%] [G loss: 0.661898]\n",
            "109 [D loss: 0.692117, acc.: 50.00%] [G loss: 0.660398]\n",
            "110 [D loss: 0.695552, acc.: 50.00%] [G loss: 0.659496]\n",
            "111 [D loss: 0.691803, acc.: 48.44%] [G loss: 0.662914]\n",
            "112 [D loss: 0.692687, acc.: 50.00%] [G loss: 0.660448]\n",
            "113 [D loss: 0.692471, acc.: 50.00%] [G loss: 0.661506]\n",
            "114 [D loss: 0.692700, acc.: 48.44%] [G loss: 0.661350]\n",
            "115 [D loss: 0.689095, acc.: 50.00%] [G loss: 0.662656]\n",
            "116 [D loss: 0.691545, acc.: 50.00%] [G loss: 0.662564]\n",
            "117 [D loss: 0.696297, acc.: 50.00%] [G loss: 0.662052]\n",
            "118 [D loss: 0.691598, acc.: 50.00%] [G loss: 0.661814]\n",
            "119 [D loss: 0.691703, acc.: 48.44%] [G loss: 0.659462]\n",
            "120 [D loss: 0.692357, acc.: 50.00%] [G loss: 0.661589]\n",
            "121 [D loss: 0.693452, acc.: 48.44%] [G loss: 0.658756]\n",
            "122 [D loss: 0.693668, acc.: 50.00%] [G loss: 0.661403]\n",
            "123 [D loss: 0.693408, acc.: 50.00%] [G loss: 0.664064]\n",
            "124 [D loss: 0.692709, acc.: 50.00%] [G loss: 0.664151]\n",
            "125 [D loss: 0.692281, acc.: 51.56%] [G loss: 0.661646]\n",
            "126 [D loss: 0.694687, acc.: 50.00%] [G loss: 0.660958]\n",
            "127 [D loss: 0.695101, acc.: 48.44%] [G loss: 0.663826]\n",
            "128 [D loss: 0.693541, acc.: 48.44%] [G loss: 0.662472]\n",
            "129 [D loss: 0.692070, acc.: 50.00%] [G loss: 0.660335]\n",
            "130 [D loss: 0.692644, acc.: 50.00%] [G loss: 0.663729]\n",
            "131 [D loss: 0.692858, acc.: 50.00%] [G loss: 0.662853]\n",
            "132 [D loss: 0.692253, acc.: 50.00%] [G loss: 0.661967]\n",
            "133 [D loss: 0.691921, acc.: 50.00%] [G loss: 0.662016]\n",
            "134 [D loss: 0.694122, acc.: 48.44%] [G loss: 0.659940]\n",
            "135 [D loss: 0.693249, acc.: 45.31%] [G loss: 0.662254]\n",
            "136 [D loss: 0.691804, acc.: 50.00%] [G loss: 0.660256]\n",
            "137 [D loss: 0.691803, acc.: 50.00%] [G loss: 0.660504]\n",
            "138 [D loss: 0.691469, acc.: 50.00%] [G loss: 0.663191]\n",
            "139 [D loss: 0.692125, acc.: 48.44%] [G loss: 0.662322]\n",
            "140 [D loss: 0.696416, acc.: 50.00%] [G loss: 0.663068]\n",
            "141 [D loss: 0.694046, acc.: 50.00%] [G loss: 0.660049]\n",
            "142 [D loss: 0.693192, acc.: 50.00%] [G loss: 0.664031]\n",
            "143 [D loss: 0.692166, acc.: 50.00%] [G loss: 0.663173]\n",
            "144 [D loss: 0.694371, acc.: 50.00%] [G loss: 0.661394]\n",
            "145 [D loss: 0.694569, acc.: 50.00%] [G loss: 0.662616]\n",
            "146 [D loss: 0.691484, acc.: 50.00%] [G loss: 0.661372]\n",
            "147 [D loss: 0.696548, acc.: 48.44%] [G loss: 0.661661]\n",
            "148 [D loss: 0.692558, acc.: 48.44%] [G loss: 0.663087]\n",
            "149 [D loss: 0.692239, acc.: 50.00%] [G loss: 0.662440]\n",
            "150 [D loss: 0.691145, acc.: 50.00%] [G loss: 0.665374]\n",
            "151 [D loss: 0.691273, acc.: 50.00%] [G loss: 0.662115]\n",
            "152 [D loss: 0.695511, acc.: 48.44%] [G loss: 0.662089]\n",
            "153 [D loss: 0.690160, acc.: 50.00%] [G loss: 0.661963]\n",
            "154 [D loss: 0.693709, acc.: 50.00%] [G loss: 0.663244]\n",
            "155 [D loss: 0.693710, acc.: 48.44%] [G loss: 0.663016]\n",
            "156 [D loss: 0.692684, acc.: 50.00%] [G loss: 0.661712]\n",
            "157 [D loss: 0.695943, acc.: 48.44%] [G loss: 0.661080]\n",
            "158 [D loss: 0.691046, acc.: 50.00%] [G loss: 0.662430]\n",
            "159 [D loss: 0.691791, acc.: 48.44%] [G loss: 0.662526]\n",
            "160 [D loss: 0.690808, acc.: 50.00%] [G loss: 0.663067]\n",
            "161 [D loss: 0.691944, acc.: 48.44%] [G loss: 0.661631]\n",
            "162 [D loss: 0.694590, acc.: 48.44%] [G loss: 0.664047]\n",
            "163 [D loss: 0.692172, acc.: 50.00%] [G loss: 0.662046]\n",
            "164 [D loss: 0.689406, acc.: 50.00%] [G loss: 0.661812]\n",
            "165 [D loss: 0.692889, acc.: 50.00%] [G loss: 0.662126]\n",
            "166 [D loss: 0.693876, acc.: 48.44%] [G loss: 0.666070]\n",
            "167 [D loss: 0.692722, acc.: 50.00%] [G loss: 0.664114]\n",
            "168 [D loss: 0.694875, acc.: 50.00%] [G loss: 0.663411]\n",
            "169 [D loss: 0.693574, acc.: 50.00%] [G loss: 0.660043]\n",
            "170 [D loss: 0.693666, acc.: 48.44%] [G loss: 0.663165]\n",
            "171 [D loss: 0.694017, acc.: 48.44%] [G loss: 0.664437]\n",
            "172 [D loss: 0.694452, acc.: 50.00%] [G loss: 0.663108]\n",
            "173 [D loss: 0.694289, acc.: 50.00%] [G loss: 0.663599]\n",
            "174 [D loss: 0.695287, acc.: 48.44%] [G loss: 0.662734]\n",
            "175 [D loss: 0.691014, acc.: 50.00%] [G loss: 0.662202]\n",
            "176 [D loss: 0.691980, acc.: 50.00%] [G loss: 0.659641]\n",
            "177 [D loss: 0.690323, acc.: 50.00%] [G loss: 0.661968]\n",
            "178 [D loss: 0.692198, acc.: 48.44%] [G loss: 0.661353]\n",
            "179 [D loss: 0.693745, acc.: 46.88%] [G loss: 0.662719]\n",
            "180 [D loss: 0.693877, acc.: 50.00%] [G loss: 0.662176]\n",
            "181 [D loss: 0.693003, acc.: 46.88%] [G loss: 0.662405]\n",
            "182 [D loss: 0.694253, acc.: 50.00%] [G loss: 0.662577]\n",
            "183 [D loss: 0.691338, acc.: 50.00%] [G loss: 0.660019]\n",
            "184 [D loss: 0.695014, acc.: 50.00%] [G loss: 0.662505]\n",
            "185 [D loss: 0.695652, acc.: 46.88%] [G loss: 0.664328]\n",
            "186 [D loss: 0.687941, acc.: 50.00%] [G loss: 0.662107]\n",
            "187 [D loss: 0.690412, acc.: 50.00%] [G loss: 0.660377]\n",
            "188 [D loss: 0.693412, acc.: 50.00%] [G loss: 0.659793]\n",
            "189 [D loss: 0.693683, acc.: 50.00%] [G loss: 0.661858]\n",
            "190 [D loss: 0.691453, acc.: 50.00%] [G loss: 0.662343]\n",
            "191 [D loss: 0.692993, acc.: 48.44%] [G loss: 0.661334]\n",
            "192 [D loss: 0.692813, acc.: 50.00%] [G loss: 0.662622]\n",
            "193 [D loss: 0.690770, acc.: 50.00%] [G loss: 0.660077]\n",
            "194 [D loss: 0.693995, acc.: 50.00%] [G loss: 0.661295]\n",
            "195 [D loss: 0.692151, acc.: 50.00%] [G loss: 0.661338]\n",
            "196 [D loss: 0.691932, acc.: 50.00%] [G loss: 0.660036]\n",
            "197 [D loss: 0.692442, acc.: 46.88%] [G loss: 0.663291]\n",
            "198 [D loss: 0.689766, acc.: 51.56%] [G loss: 0.661583]\n",
            "199 [D loss: 0.689507, acc.: 50.00%] [G loss: 0.663036]\n",
            "200 [D loss: 0.693117, acc.: 48.44%] [G loss: 0.662797]\n",
            "201 [D loss: 0.690508, acc.: 48.44%] [G loss: 0.660952]\n",
            "202 [D loss: 0.690949, acc.: 50.00%] [G loss: 0.661117]\n",
            "203 [D loss: 0.691955, acc.: 50.00%] [G loss: 0.663496]\n",
            "204 [D loss: 0.690306, acc.: 48.44%] [G loss: 0.660554]\n",
            "205 [D loss: 0.695290, acc.: 48.44%] [G loss: 0.662833]\n",
            "206 [D loss: 0.691511, acc.: 48.44%] [G loss: 0.662153]\n",
            "207 [D loss: 0.689165, acc.: 50.00%] [G loss: 0.659427]\n",
            "208 [D loss: 0.694899, acc.: 48.44%] [G loss: 0.660524]\n",
            "209 [D loss: 0.692675, acc.: 48.44%] [G loss: 0.663694]\n",
            "210 [D loss: 0.693013, acc.: 48.44%] [G loss: 0.663862]\n",
            "211 [D loss: 0.691807, acc.: 50.00%] [G loss: 0.660850]\n",
            "212 [D loss: 0.694063, acc.: 50.00%] [G loss: 0.660324]\n",
            "213 [D loss: 0.691691, acc.: 48.44%] [G loss: 0.663360]\n",
            "214 [D loss: 0.691905, acc.: 50.00%] [G loss: 0.662709]\n",
            "215 [D loss: 0.691986, acc.: 50.00%] [G loss: 0.662209]\n",
            "216 [D loss: 0.694021, acc.: 50.00%] [G loss: 0.662594]\n",
            "217 [D loss: 0.692557, acc.: 50.00%] [G loss: 0.664177]\n",
            "218 [D loss: 0.692478, acc.: 50.00%] [G loss: 0.662488]\n",
            "219 [D loss: 0.693864, acc.: 50.00%] [G loss: 0.664794]\n",
            "220 [D loss: 0.691781, acc.: 50.00%] [G loss: 0.664883]\n",
            "221 [D loss: 0.694725, acc.: 50.00%] [G loss: 0.662176]\n",
            "222 [D loss: 0.690226, acc.: 50.00%] [G loss: 0.661696]\n",
            "223 [D loss: 0.692702, acc.: 50.00%] [G loss: 0.663408]\n",
            "224 [D loss: 0.692890, acc.: 50.00%] [G loss: 0.662965]\n",
            "225 [D loss: 0.693524, acc.: 48.44%] [G loss: 0.662223]\n",
            "226 [D loss: 0.691718, acc.: 48.44%] [G loss: 0.660854]\n",
            "227 [D loss: 0.693082, acc.: 48.44%] [G loss: 0.661717]\n",
            "228 [D loss: 0.692156, acc.: 50.00%] [G loss: 0.659350]\n",
            "229 [D loss: 0.693418, acc.: 50.00%] [G loss: 0.662377]\n",
            "230 [D loss: 0.693002, acc.: 50.00%] [G loss: 0.660768]\n",
            "231 [D loss: 0.690777, acc.: 50.00%] [G loss: 0.661403]\n",
            "232 [D loss: 0.692588, acc.: 50.00%] [G loss: 0.661535]\n",
            "233 [D loss: 0.691433, acc.: 46.88%] [G loss: 0.665803]\n",
            "234 [D loss: 0.692252, acc.: 50.00%] [G loss: 0.663344]\n",
            "235 [D loss: 0.694004, acc.: 50.00%] [G loss: 0.663811]\n",
            "236 [D loss: 0.693119, acc.: 48.44%] [G loss: 0.661277]\n",
            "237 [D loss: 0.693708, acc.: 50.00%] [G loss: 0.663909]\n",
            "238 [D loss: 0.692277, acc.: 48.44%] [G loss: 0.664093]\n",
            "239 [D loss: 0.692210, acc.: 50.00%] [G loss: 0.661308]\n",
            "240 [D loss: 0.690231, acc.: 50.00%] [G loss: 0.663016]\n",
            "241 [D loss: 0.691456, acc.: 50.00%] [G loss: 0.660977]\n",
            "242 [D loss: 0.690461, acc.: 50.00%] [G loss: 0.662994]\n",
            "243 [D loss: 0.691005, acc.: 50.00%] [G loss: 0.662709]\n",
            "244 [D loss: 0.693233, acc.: 48.44%] [G loss: 0.660323]\n",
            "245 [D loss: 0.693669, acc.: 48.44%] [G loss: 0.664806]\n",
            "246 [D loss: 0.693436, acc.: 50.00%] [G loss: 0.660869]\n",
            "247 [D loss: 0.693602, acc.: 48.44%] [G loss: 0.662706]\n",
            "248 [D loss: 0.692136, acc.: 50.00%] [G loss: 0.662092]\n",
            "249 [D loss: 0.692985, acc.: 48.44%] [G loss: 0.660707]\n",
            "250 [D loss: 0.694629, acc.: 48.44%] [G loss: 0.664005]\n",
            "251 [D loss: 0.692190, acc.: 50.00%] [G loss: 0.663104]\n",
            "252 [D loss: 0.693477, acc.: 46.88%] [G loss: 0.662471]\n",
            "253 [D loss: 0.691043, acc.: 48.44%] [G loss: 0.662674]\n",
            "254 [D loss: 0.692942, acc.: 50.00%] [G loss: 0.662373]\n",
            "255 [D loss: 0.689821, acc.: 48.44%] [G loss: 0.661689]\n",
            "256 [D loss: 0.693132, acc.: 46.88%] [G loss: 0.662867]\n",
            "257 [D loss: 0.692490, acc.: 45.31%] [G loss: 0.663810]\n",
            "258 [D loss: 0.690264, acc.: 50.00%] [G loss: 0.663289]\n",
            "259 [D loss: 0.692333, acc.: 51.56%] [G loss: 0.662486]\n",
            "260 [D loss: 0.695390, acc.: 50.00%] [G loss: 0.664278]\n",
            "261 [D loss: 0.693997, acc.: 50.00%] [G loss: 0.662830]\n",
            "262 [D loss: 0.692461, acc.: 50.00%] [G loss: 0.661123]\n",
            "263 [D loss: 0.693748, acc.: 50.00%] [G loss: 0.662503]\n",
            "264 [D loss: 0.692439, acc.: 51.56%] [G loss: 0.660786]\n",
            "265 [D loss: 0.692555, acc.: 48.44%] [G loss: 0.662437]\n",
            "266 [D loss: 0.694607, acc.: 50.00%] [G loss: 0.662951]\n",
            "267 [D loss: 0.694445, acc.: 45.31%] [G loss: 0.664809]\n",
            "268 [D loss: 0.693374, acc.: 48.44%] [G loss: 0.663755]\n",
            "269 [D loss: 0.691021, acc.: 50.00%] [G loss: 0.660888]\n",
            "270 [D loss: 0.691083, acc.: 48.44%] [G loss: 0.661367]\n",
            "271 [D loss: 0.693237, acc.: 50.00%] [G loss: 0.662230]\n",
            "272 [D loss: 0.692380, acc.: 50.00%] [G loss: 0.661691]\n",
            "273 [D loss: 0.693150, acc.: 48.44%] [G loss: 0.660712]\n",
            "274 [D loss: 0.692534, acc.: 50.00%] [G loss: 0.663280]\n",
            "275 [D loss: 0.690781, acc.: 48.44%] [G loss: 0.662421]\n",
            "276 [D loss: 0.689943, acc.: 48.44%] [G loss: 0.661181]\n",
            "277 [D loss: 0.693704, acc.: 50.00%] [G loss: 0.663774]\n",
            "278 [D loss: 0.693921, acc.: 50.00%] [G loss: 0.666699]\n",
            "279 [D loss: 0.689598, acc.: 50.00%] [G loss: 0.661245]\n",
            "280 [D loss: 0.691538, acc.: 50.00%] [G loss: 0.663332]\n",
            "281 [D loss: 0.692565, acc.: 50.00%] [G loss: 0.664815]\n",
            "282 [D loss: 0.691350, acc.: 50.00%] [G loss: 0.661889]\n",
            "283 [D loss: 0.692614, acc.: 48.44%] [G loss: 0.660447]\n",
            "284 [D loss: 0.693735, acc.: 50.00%] [G loss: 0.659196]\n",
            "285 [D loss: 0.694171, acc.: 50.00%] [G loss: 0.664392]\n",
            "286 [D loss: 0.692497, acc.: 48.44%] [G loss: 0.660036]\n",
            "287 [D loss: 0.690466, acc.: 50.00%] [G loss: 0.662784]\n",
            "288 [D loss: 0.694109, acc.: 50.00%] [G loss: 0.663475]\n",
            "289 [D loss: 0.692853, acc.: 48.44%] [G loss: 0.663117]\n",
            "290 [D loss: 0.693178, acc.: 50.00%] [G loss: 0.661552]\n",
            "291 [D loss: 0.690977, acc.: 50.00%] [G loss: 0.666745]\n",
            "292 [D loss: 0.691376, acc.: 50.00%] [G loss: 0.665904]\n",
            "293 [D loss: 0.691165, acc.: 50.00%] [G loss: 0.664822]\n",
            "294 [D loss: 0.691810, acc.: 50.00%] [G loss: 0.665600]\n",
            "295 [D loss: 0.696529, acc.: 48.44%] [G loss: 0.664760]\n",
            "296 [D loss: 0.693391, acc.: 45.31%] [G loss: 0.664998]\n",
            "297 [D loss: 0.692424, acc.: 50.00%] [G loss: 0.664503]\n",
            "298 [D loss: 0.692475, acc.: 50.00%] [G loss: 0.663445]\n",
            "299 [D loss: 0.691031, acc.: 50.00%] [G loss: 0.662027]\n",
            "300 [D loss: 0.692351, acc.: 50.00%] [G loss: 0.663779]\n",
            "301 [D loss: 0.689896, acc.: 50.00%] [G loss: 0.664556]\n",
            "302 [D loss: 0.695354, acc.: 46.88%] [G loss: 0.664140]\n",
            "303 [D loss: 0.693282, acc.: 50.00%] [G loss: 0.662579]\n",
            "304 [D loss: 0.692138, acc.: 48.44%] [G loss: 0.661793]\n",
            "305 [D loss: 0.696324, acc.: 50.00%] [G loss: 0.661728]\n",
            "306 [D loss: 0.692734, acc.: 50.00%] [G loss: 0.663029]\n",
            "307 [D loss: 0.691190, acc.: 50.00%] [G loss: 0.661822]\n",
            "308 [D loss: 0.692302, acc.: 50.00%] [G loss: 0.663394]\n",
            "309 [D loss: 0.691572, acc.: 50.00%] [G loss: 0.663615]\n",
            "310 [D loss: 0.692125, acc.: 50.00%] [G loss: 0.663894]\n",
            "311 [D loss: 0.692931, acc.: 48.44%] [G loss: 0.662252]\n",
            "312 [D loss: 0.691262, acc.: 50.00%] [G loss: 0.664166]\n",
            "313 [D loss: 0.691780, acc.: 50.00%] [G loss: 0.661655]\n",
            "314 [D loss: 0.690999, acc.: 50.00%] [G loss: 0.661141]\n",
            "315 [D loss: 0.696604, acc.: 48.44%] [G loss: 0.662291]\n",
            "316 [D loss: 0.693719, acc.: 46.88%] [G loss: 0.662851]\n",
            "317 [D loss: 0.690674, acc.: 48.44%] [G loss: 0.662768]\n",
            "318 [D loss: 0.689964, acc.: 50.00%] [G loss: 0.662989]\n",
            "319 [D loss: 0.692057, acc.: 48.44%] [G loss: 0.662444]\n",
            "320 [D loss: 0.691848, acc.: 50.00%] [G loss: 0.661914]\n",
            "321 [D loss: 0.692627, acc.: 50.00%] [G loss: 0.662326]\n",
            "322 [D loss: 0.690955, acc.: 50.00%] [G loss: 0.661657]\n",
            "323 [D loss: 0.691464, acc.: 48.44%] [G loss: 0.663601]\n",
            "324 [D loss: 0.692971, acc.: 48.44%] [G loss: 0.662864]\n",
            "325 [D loss: 0.690885, acc.: 48.44%] [G loss: 0.662021]\n",
            "326 [D loss: 0.687181, acc.: 50.00%] [G loss: 0.665631]\n",
            "327 [D loss: 0.689682, acc.: 50.00%] [G loss: 0.664706]\n",
            "328 [D loss: 0.690993, acc.: 48.44%] [G loss: 0.663851]\n",
            "329 [D loss: 0.689442, acc.: 50.00%] [G loss: 0.662719]\n",
            "330 [D loss: 0.692011, acc.: 50.00%] [G loss: 0.662679]\n",
            "331 [D loss: 0.692484, acc.: 46.88%] [G loss: 0.662980]\n",
            "332 [D loss: 0.691164, acc.: 48.44%] [G loss: 0.664792]\n",
            "333 [D loss: 0.693131, acc.: 46.88%] [G loss: 0.664771]\n",
            "334 [D loss: 0.691221, acc.: 50.00%] [G loss: 0.662076]\n",
            "335 [D loss: 0.694322, acc.: 48.44%] [G loss: 0.663769]\n",
            "336 [D loss: 0.691889, acc.: 50.00%] [G loss: 0.662880]\n",
            "337 [D loss: 0.690482, acc.: 50.00%] [G loss: 0.661789]\n",
            "338 [D loss: 0.693233, acc.: 48.44%] [G loss: 0.664372]\n",
            "339 [D loss: 0.690486, acc.: 50.00%] [G loss: 0.663227]\n",
            "340 [D loss: 0.692115, acc.: 48.44%] [G loss: 0.664742]\n",
            "341 [D loss: 0.691960, acc.: 48.44%] [G loss: 0.665001]\n",
            "342 [D loss: 0.693493, acc.: 50.00%] [G loss: 0.663469]\n",
            "343 [D loss: 0.691204, acc.: 48.44%] [G loss: 0.662655]\n",
            "344 [D loss: 0.693682, acc.: 48.44%] [G loss: 0.662008]\n",
            "345 [D loss: 0.691833, acc.: 51.56%] [G loss: 0.661740]\n",
            "346 [D loss: 0.692230, acc.: 46.88%] [G loss: 0.663281]\n",
            "347 [D loss: 0.692453, acc.: 50.00%] [G loss: 0.661749]\n",
            "348 [D loss: 0.693162, acc.: 50.00%] [G loss: 0.662477]\n",
            "349 [D loss: 0.691418, acc.: 50.00%] [G loss: 0.662373]\n",
            "350 [D loss: 0.689023, acc.: 50.00%] [G loss: 0.664315]\n",
            "351 [D loss: 0.691945, acc.: 50.00%] [G loss: 0.658970]\n",
            "352 [D loss: 0.693501, acc.: 48.44%] [G loss: 0.663562]\n",
            "353 [D loss: 0.693703, acc.: 48.44%] [G loss: 0.662952]\n",
            "354 [D loss: 0.691647, acc.: 50.00%] [G loss: 0.662212]\n",
            "355 [D loss: 0.691551, acc.: 50.00%] [G loss: 0.663836]\n",
            "356 [D loss: 0.692665, acc.: 48.44%] [G loss: 0.662994]\n",
            "357 [D loss: 0.690491, acc.: 50.00%] [G loss: 0.663426]\n",
            "358 [D loss: 0.694971, acc.: 48.44%] [G loss: 0.665533]\n",
            "359 [D loss: 0.695319, acc.: 45.31%] [G loss: 0.664656]\n",
            "360 [D loss: 0.695737, acc.: 46.88%] [G loss: 0.661644]\n",
            "361 [D loss: 0.691195, acc.: 50.00%] [G loss: 0.662670]\n",
            "362 [D loss: 0.692448, acc.: 50.00%] [G loss: 0.663433]\n",
            "363 [D loss: 0.694677, acc.: 45.31%] [G loss: 0.661597]\n",
            "364 [D loss: 0.691798, acc.: 48.44%] [G loss: 0.662603]\n",
            "365 [D loss: 0.691813, acc.: 48.44%] [G loss: 0.660051]\n",
            "366 [D loss: 0.690427, acc.: 50.00%] [G loss: 0.660980]\n",
            "367 [D loss: 0.691606, acc.: 48.44%] [G loss: 0.660025]\n",
            "368 [D loss: 0.690013, acc.: 50.00%] [G loss: 0.664968]\n",
            "369 [D loss: 0.690799, acc.: 50.00%] [G loss: 0.665112]\n",
            "370 [D loss: 0.691129, acc.: 50.00%] [G loss: 0.662577]\n",
            "371 [D loss: 0.691411, acc.: 50.00%] [G loss: 0.663846]\n",
            "372 [D loss: 0.691504, acc.: 48.44%] [G loss: 0.660152]\n",
            "373 [D loss: 0.691699, acc.: 50.00%] [G loss: 0.663286]\n",
            "374 [D loss: 0.692326, acc.: 48.44%] [G loss: 0.663066]\n",
            "375 [D loss: 0.690500, acc.: 50.00%] [G loss: 0.663466]\n",
            "376 [D loss: 0.692430, acc.: 50.00%] [G loss: 0.660382]\n",
            "377 [D loss: 0.693466, acc.: 50.00%] [G loss: 0.663230]\n",
            "378 [D loss: 0.690544, acc.: 50.00%] [G loss: 0.662688]\n",
            "379 [D loss: 0.691782, acc.: 48.44%] [G loss: 0.662404]\n",
            "380 [D loss: 0.693406, acc.: 50.00%] [G loss: 0.663790]\n",
            "381 [D loss: 0.692775, acc.: 50.00%] [G loss: 0.661250]\n",
            "382 [D loss: 0.693505, acc.: 50.00%] [G loss: 0.663573]\n",
            "383 [D loss: 0.690176, acc.: 50.00%] [G loss: 0.661291]\n",
            "384 [D loss: 0.692308, acc.: 50.00%] [G loss: 0.660756]\n",
            "385 [D loss: 0.692738, acc.: 48.44%] [G loss: 0.663190]\n",
            "386 [D loss: 0.689854, acc.: 50.00%] [G loss: 0.663952]\n",
            "387 [D loss: 0.691877, acc.: 48.44%] [G loss: 0.664534]\n",
            "388 [D loss: 0.694219, acc.: 48.44%] [G loss: 0.661645]\n",
            "389 [D loss: 0.692010, acc.: 48.44%] [G loss: 0.666111]\n",
            "390 [D loss: 0.691715, acc.: 50.00%] [G loss: 0.665267]\n",
            "391 [D loss: 0.693830, acc.: 48.44%] [G loss: 0.664950]\n",
            "392 [D loss: 0.693890, acc.: 48.44%] [G loss: 0.662098]\n",
            "393 [D loss: 0.692153, acc.: 51.56%] [G loss: 0.665899]\n",
            "394 [D loss: 0.692756, acc.: 50.00%] [G loss: 0.663891]\n",
            "395 [D loss: 0.693799, acc.: 48.44%] [G loss: 0.665755]\n",
            "396 [D loss: 0.693086, acc.: 48.44%] [G loss: 0.668030]\n",
            "397 [D loss: 0.694806, acc.: 48.44%] [G loss: 0.662412]\n",
            "398 [D loss: 0.691583, acc.: 50.00%] [G loss: 0.662703]\n",
            "399 [D loss: 0.692465, acc.: 50.00%] [G loss: 0.663729]\n",
            "400 [D loss: 0.691003, acc.: 46.88%] [G loss: 0.660845]\n",
            "401 [D loss: 0.692849, acc.: 48.44%] [G loss: 0.663270]\n",
            "402 [D loss: 0.691529, acc.: 46.88%] [G loss: 0.661820]\n",
            "403 [D loss: 0.691951, acc.: 48.44%] [G loss: 0.662304]\n",
            "404 [D loss: 0.692902, acc.: 46.88%] [G loss: 0.663359]\n",
            "405 [D loss: 0.693098, acc.: 50.00%] [G loss: 0.663973]\n",
            "406 [D loss: 0.692079, acc.: 50.00%] [G loss: 0.664624]\n",
            "407 [D loss: 0.691137, acc.: 50.00%] [G loss: 0.662968]\n",
            "408 [D loss: 0.692547, acc.: 46.88%] [G loss: 0.664195]\n",
            "409 [D loss: 0.692661, acc.: 50.00%] [G loss: 0.661825]\n",
            "410 [D loss: 0.691828, acc.: 50.00%] [G loss: 0.661695]\n",
            "411 [D loss: 0.690493, acc.: 50.00%] [G loss: 0.665074]\n",
            "412 [D loss: 0.688222, acc.: 50.00%] [G loss: 0.664307]\n",
            "413 [D loss: 0.692113, acc.: 50.00%] [G loss: 0.662442]\n",
            "414 [D loss: 0.692576, acc.: 50.00%] [G loss: 0.664509]\n",
            "415 [D loss: 0.692805, acc.: 48.44%] [G loss: 0.661739]\n",
            "416 [D loss: 0.689895, acc.: 50.00%] [G loss: 0.662457]\n",
            "417 [D loss: 0.691615, acc.: 50.00%] [G loss: 0.662489]\n",
            "418 [D loss: 0.693904, acc.: 46.88%] [G loss: 0.662719]\n",
            "419 [D loss: 0.692434, acc.: 48.44%] [G loss: 0.664038]\n",
            "420 [D loss: 0.690896, acc.: 50.00%] [G loss: 0.664869]\n",
            "421 [D loss: 0.691286, acc.: 50.00%] [G loss: 0.663451]\n",
            "422 [D loss: 0.691409, acc.: 50.00%] [G loss: 0.663208]\n",
            "423 [D loss: 0.692679, acc.: 50.00%] [G loss: 0.664409]\n",
            "424 [D loss: 0.691581, acc.: 48.44%] [G loss: 0.666224]\n",
            "425 [D loss: 0.690955, acc.: 48.44%] [G loss: 0.661343]\n",
            "426 [D loss: 0.694365, acc.: 48.44%] [G loss: 0.661519]\n",
            "427 [D loss: 0.690253, acc.: 50.00%] [G loss: 0.662720]\n",
            "428 [D loss: 0.690485, acc.: 50.00%] [G loss: 0.663229]\n",
            "429 [D loss: 0.691037, acc.: 50.00%] [G loss: 0.664688]\n",
            "430 [D loss: 0.692297, acc.: 50.00%] [G loss: 0.660387]\n",
            "431 [D loss: 0.692343, acc.: 48.44%] [G loss: 0.664203]\n",
            "432 [D loss: 0.693412, acc.: 48.44%] [G loss: 0.666888]\n",
            "433 [D loss: 0.692242, acc.: 50.00%] [G loss: 0.662608]\n",
            "434 [D loss: 0.691130, acc.: 46.88%] [G loss: 0.664935]\n",
            "435 [D loss: 0.693438, acc.: 48.44%] [G loss: 0.661814]\n",
            "436 [D loss: 0.694266, acc.: 48.44%] [G loss: 0.661862]\n",
            "437 [D loss: 0.691211, acc.: 48.44%] [G loss: 0.662929]\n",
            "438 [D loss: 0.692987, acc.: 48.44%] [G loss: 0.662854]\n",
            "439 [D loss: 0.696318, acc.: 48.44%] [G loss: 0.662064]\n",
            "440 [D loss: 0.694170, acc.: 48.44%] [G loss: 0.662826]\n",
            "441 [D loss: 0.694564, acc.: 50.00%] [G loss: 0.662999]\n",
            "442 [D loss: 0.694083, acc.: 50.00%] [G loss: 0.661835]\n",
            "443 [D loss: 0.692539, acc.: 48.44%] [G loss: 0.662280]\n",
            "444 [D loss: 0.690602, acc.: 50.00%] [G loss: 0.664998]\n",
            "445 [D loss: 0.692063, acc.: 48.44%] [G loss: 0.663618]\n",
            "446 [D loss: 0.690708, acc.: 50.00%] [G loss: 0.664839]\n",
            "447 [D loss: 0.694962, acc.: 48.44%] [G loss: 0.666663]\n",
            "448 [D loss: 0.689942, acc.: 48.44%] [G loss: 0.662703]\n",
            "449 [D loss: 0.691937, acc.: 50.00%] [G loss: 0.662601]\n",
            "450 [D loss: 0.694143, acc.: 50.00%] [G loss: 0.663225]\n",
            "451 [D loss: 0.691634, acc.: 50.00%] [G loss: 0.662753]\n",
            "452 [D loss: 0.690340, acc.: 50.00%] [G loss: 0.664041]\n",
            "453 [D loss: 0.693661, acc.: 48.44%] [G loss: 0.666155]\n",
            "454 [D loss: 0.693562, acc.: 50.00%] [G loss: 0.663736]\n",
            "455 [D loss: 0.691872, acc.: 50.00%] [G loss: 0.663205]\n",
            "456 [D loss: 0.689996, acc.: 50.00%] [G loss: 0.665628]\n",
            "457 [D loss: 0.691097, acc.: 50.00%] [G loss: 0.662679]\n",
            "458 [D loss: 0.691059, acc.: 48.44%] [G loss: 0.664672]\n",
            "459 [D loss: 0.691143, acc.: 50.00%] [G loss: 0.665159]\n",
            "460 [D loss: 0.691825, acc.: 50.00%] [G loss: 0.662621]\n",
            "461 [D loss: 0.689615, acc.: 50.00%] [G loss: 0.663153]\n",
            "462 [D loss: 0.692409, acc.: 50.00%] [G loss: 0.664428]\n",
            "463 [D loss: 0.692047, acc.: 50.00%] [G loss: 0.664168]\n",
            "464 [D loss: 0.693458, acc.: 48.44%] [G loss: 0.664548]\n",
            "465 [D loss: 0.694111, acc.: 50.00%] [G loss: 0.663091]\n",
            "466 [D loss: 0.693900, acc.: 46.88%] [G loss: 0.666930]\n",
            "467 [D loss: 0.689923, acc.: 50.00%] [G loss: 0.663206]\n",
            "468 [D loss: 0.694829, acc.: 48.44%] [G loss: 0.663271]\n",
            "469 [D loss: 0.691847, acc.: 48.44%] [G loss: 0.663001]\n",
            "470 [D loss: 0.689563, acc.: 50.00%] [G loss: 0.661569]\n",
            "471 [D loss: 0.695367, acc.: 50.00%] [G loss: 0.662513]\n",
            "472 [D loss: 0.692833, acc.: 50.00%] [G loss: 0.664094]\n",
            "473 [D loss: 0.693487, acc.: 48.44%] [G loss: 0.663211]\n",
            "474 [D loss: 0.692184, acc.: 46.88%] [G loss: 0.663553]\n",
            "475 [D loss: 0.692771, acc.: 50.00%] [G loss: 0.665087]\n",
            "476 [D loss: 0.694604, acc.: 50.00%] [G loss: 0.662186]\n",
            "477 [D loss: 0.692351, acc.: 45.31%] [G loss: 0.663116]\n",
            "478 [D loss: 0.693964, acc.: 48.44%] [G loss: 0.665775]\n",
            "479 [D loss: 0.692119, acc.: 50.00%] [G loss: 0.661981]\n",
            "480 [D loss: 0.691205, acc.: 48.44%] [G loss: 0.665842]\n",
            "481 [D loss: 0.693327, acc.: 50.00%] [G loss: 0.665229]\n",
            "482 [D loss: 0.692479, acc.: 48.44%] [G loss: 0.664339]\n",
            "483 [D loss: 0.691672, acc.: 48.44%] [G loss: 0.661404]\n",
            "484 [D loss: 0.693114, acc.: 48.44%] [G loss: 0.664360]\n",
            "485 [D loss: 0.691523, acc.: 50.00%] [G loss: 0.664683]\n",
            "486 [D loss: 0.693641, acc.: 50.00%] [G loss: 0.663938]\n",
            "487 [D loss: 0.690844, acc.: 50.00%] [G loss: 0.661096]\n",
            "488 [D loss: 0.694905, acc.: 48.44%] [G loss: 0.664984]\n",
            "489 [D loss: 0.692200, acc.: 50.00%] [G loss: 0.663808]\n",
            "490 [D loss: 0.692718, acc.: 50.00%] [G loss: 0.663917]\n",
            "491 [D loss: 0.695438, acc.: 50.00%] [G loss: 0.663369]\n",
            "492 [D loss: 0.691305, acc.: 48.44%] [G loss: 0.661849]\n",
            "493 [D loss: 0.692230, acc.: 50.00%] [G loss: 0.662721]\n",
            "494 [D loss: 0.693975, acc.: 48.44%] [G loss: 0.667502]\n",
            "495 [D loss: 0.690361, acc.: 51.56%] [G loss: 0.664011]\n",
            "496 [D loss: 0.693315, acc.: 50.00%] [G loss: 0.661836]\n",
            "497 [D loss: 0.694828, acc.: 48.44%] [G loss: 0.664647]\n",
            "498 [D loss: 0.694286, acc.: 46.88%] [G loss: 0.664600]\n",
            "499 [D loss: 0.691484, acc.: 50.00%] [G loss: 0.663792]\n",
            "500 [D loss: 0.693010, acc.: 48.44%] [G loss: 0.667261]\n",
            "501 [D loss: 0.692772, acc.: 50.00%] [G loss: 0.665181]\n",
            "502 [D loss: 0.694889, acc.: 48.44%] [G loss: 0.664992]\n",
            "503 [D loss: 0.690184, acc.: 48.44%] [G loss: 0.665127]\n",
            "504 [D loss: 0.692503, acc.: 48.44%] [G loss: 0.664521]\n",
            "505 [D loss: 0.690973, acc.: 46.88%] [G loss: 0.665037]\n",
            "506 [D loss: 0.693697, acc.: 48.44%] [G loss: 0.661281]\n",
            "507 [D loss: 0.691544, acc.: 50.00%] [G loss: 0.662657]\n",
            "508 [D loss: 0.693519, acc.: 50.00%] [G loss: 0.663516]\n",
            "509 [D loss: 0.692861, acc.: 48.44%] [G loss: 0.660748]\n",
            "510 [D loss: 0.690123, acc.: 50.00%] [G loss: 0.664552]\n",
            "511 [D loss: 0.689997, acc.: 50.00%] [G loss: 0.666130]\n",
            "512 [D loss: 0.692881, acc.: 48.44%] [G loss: 0.664775]\n",
            "513 [D loss: 0.692264, acc.: 48.44%] [G loss: 0.663436]\n",
            "514 [D loss: 0.691666, acc.: 48.44%] [G loss: 0.664209]\n",
            "515 [D loss: 0.692698, acc.: 48.44%] [G loss: 0.661786]\n",
            "516 [D loss: 0.690521, acc.: 50.00%] [G loss: 0.662293]\n",
            "517 [D loss: 0.692292, acc.: 48.44%] [G loss: 0.661637]\n",
            "518 [D loss: 0.693036, acc.: 48.44%] [G loss: 0.661187]\n",
            "519 [D loss: 0.689993, acc.: 50.00%] [G loss: 0.664365]\n",
            "520 [D loss: 0.692177, acc.: 48.44%] [G loss: 0.665538]\n",
            "521 [D loss: 0.694138, acc.: 48.44%] [G loss: 0.663514]\n",
            "522 [D loss: 0.693604, acc.: 50.00%] [G loss: 0.662459]\n",
            "523 [D loss: 0.690462, acc.: 48.44%] [G loss: 0.663540]\n",
            "524 [D loss: 0.694781, acc.: 48.44%] [G loss: 0.667478]\n",
            "525 [D loss: 0.691418, acc.: 50.00%] [G loss: 0.664781]\n",
            "526 [D loss: 0.691708, acc.: 50.00%] [G loss: 0.665148]\n",
            "527 [D loss: 0.693147, acc.: 48.44%] [G loss: 0.661973]\n",
            "528 [D loss: 0.692319, acc.: 50.00%] [G loss: 0.663982]\n",
            "529 [D loss: 0.693160, acc.: 50.00%] [G loss: 0.663937]\n",
            "530 [D loss: 0.691610, acc.: 50.00%] [G loss: 0.665239]\n",
            "531 [D loss: 0.691974, acc.: 48.44%] [G loss: 0.660792]\n",
            "532 [D loss: 0.692633, acc.: 50.00%] [G loss: 0.664234]\n",
            "533 [D loss: 0.689661, acc.: 51.56%] [G loss: 0.664174]\n",
            "534 [D loss: 0.693327, acc.: 48.44%] [G loss: 0.662534]\n",
            "535 [D loss: 0.688930, acc.: 50.00%] [G loss: 0.663002]\n",
            "536 [D loss: 0.691656, acc.: 50.00%] [G loss: 0.660773]\n",
            "537 [D loss: 0.689998, acc.: 50.00%] [G loss: 0.663774]\n",
            "538 [D loss: 0.689129, acc.: 48.44%] [G loss: 0.665687]\n",
            "539 [D loss: 0.694276, acc.: 50.00%] [G loss: 0.665975]\n",
            "540 [D loss: 0.694106, acc.: 50.00%] [G loss: 0.661759]\n",
            "541 [D loss: 0.692605, acc.: 50.00%] [G loss: 0.667440]\n",
            "542 [D loss: 0.691786, acc.: 48.44%] [G loss: 0.664240]\n",
            "543 [D loss: 0.689678, acc.: 48.44%] [G loss: 0.663429]\n",
            "544 [D loss: 0.691890, acc.: 48.44%] [G loss: 0.663360]\n",
            "545 [D loss: 0.690756, acc.: 50.00%] [G loss: 0.663244]\n",
            "546 [D loss: 0.691141, acc.: 48.44%] [G loss: 0.664118]\n",
            "547 [D loss: 0.689977, acc.: 50.00%] [G loss: 0.662181]\n",
            "548 [D loss: 0.691921, acc.: 50.00%] [G loss: 0.662980]\n",
            "549 [D loss: 0.692322, acc.: 50.00%] [G loss: 0.663838]\n",
            "550 [D loss: 0.690741, acc.: 50.00%] [G loss: 0.662365]\n",
            "551 [D loss: 0.692836, acc.: 50.00%] [G loss: 0.665324]\n",
            "552 [D loss: 0.693250, acc.: 46.88%] [G loss: 0.662352]\n",
            "553 [D loss: 0.693021, acc.: 48.44%] [G loss: 0.663054]\n",
            "554 [D loss: 0.692955, acc.: 48.44%] [G loss: 0.662203]\n",
            "555 [D loss: 0.692769, acc.: 48.44%] [G loss: 0.663164]\n",
            "556 [D loss: 0.690119, acc.: 48.44%] [G loss: 0.664644]\n",
            "557 [D loss: 0.687419, acc.: 50.00%] [G loss: 0.665945]\n",
            "558 [D loss: 0.692034, acc.: 50.00%] [G loss: 0.663026]\n",
            "559 [D loss: 0.692497, acc.: 48.44%] [G loss: 0.663556]\n",
            "560 [D loss: 0.695481, acc.: 48.44%] [G loss: 0.664377]\n",
            "561 [D loss: 0.692907, acc.: 50.00%] [G loss: 0.664792]\n",
            "562 [D loss: 0.689551, acc.: 50.00%] [G loss: 0.666172]\n",
            "563 [D loss: 0.691047, acc.: 50.00%] [G loss: 0.661277]\n",
            "564 [D loss: 0.692081, acc.: 50.00%] [G loss: 0.665642]\n",
            "565 [D loss: 0.690988, acc.: 48.44%] [G loss: 0.664987]\n",
            "566 [D loss: 0.687994, acc.: 50.00%] [G loss: 0.661641]\n",
            "567 [D loss: 0.693738, acc.: 50.00%] [G loss: 0.664185]\n",
            "568 [D loss: 0.693177, acc.: 51.56%] [G loss: 0.663292]\n",
            "569 [D loss: 0.692053, acc.: 50.00%] [G loss: 0.664487]\n",
            "570 [D loss: 0.693303, acc.: 50.00%] [G loss: 0.664093]\n",
            "571 [D loss: 0.691789, acc.: 48.44%] [G loss: 0.665511]\n",
            "572 [D loss: 0.692377, acc.: 46.88%] [G loss: 0.662395]\n",
            "573 [D loss: 0.688416, acc.: 50.00%] [G loss: 0.661354]\n",
            "574 [D loss: 0.690202, acc.: 48.44%] [G loss: 0.665265]\n",
            "575 [D loss: 0.694584, acc.: 50.00%] [G loss: 0.664998]\n",
            "576 [D loss: 0.692580, acc.: 50.00%] [G loss: 0.667297]\n",
            "577 [D loss: 0.693609, acc.: 50.00%] [G loss: 0.663393]\n",
            "578 [D loss: 0.691617, acc.: 50.00%] [G loss: 0.663824]\n",
            "579 [D loss: 0.692262, acc.: 46.88%] [G loss: 0.661364]\n",
            "580 [D loss: 0.693747, acc.: 48.44%] [G loss: 0.663958]\n",
            "581 [D loss: 0.693430, acc.: 46.88%] [G loss: 0.664683]\n",
            "582 [D loss: 0.691384, acc.: 50.00%] [G loss: 0.662245]\n",
            "583 [D loss: 0.691838, acc.: 50.00%] [G loss: 0.665677]\n",
            "584 [D loss: 0.688428, acc.: 50.00%] [G loss: 0.663367]\n",
            "585 [D loss: 0.691889, acc.: 50.00%] [G loss: 0.664546]\n",
            "586 [D loss: 0.690058, acc.: 50.00%] [G loss: 0.663422]\n",
            "587 [D loss: 0.693894, acc.: 48.44%] [G loss: 0.666435]\n",
            "588 [D loss: 0.687675, acc.: 50.00%] [G loss: 0.666068]\n",
            "589 [D loss: 0.690949, acc.: 50.00%] [G loss: 0.664889]\n",
            "590 [D loss: 0.694790, acc.: 50.00%] [G loss: 0.664712]\n",
            "591 [D loss: 0.690758, acc.: 50.00%] [G loss: 0.663938]\n",
            "592 [D loss: 0.692032, acc.: 50.00%] [G loss: 0.662709]\n",
            "593 [D loss: 0.692683, acc.: 50.00%] [G loss: 0.666960]\n",
            "594 [D loss: 0.689244, acc.: 50.00%] [G loss: 0.662758]\n",
            "595 [D loss: 0.694458, acc.: 46.88%] [G loss: 0.663352]\n",
            "596 [D loss: 0.693521, acc.: 50.00%] [G loss: 0.663061]\n",
            "597 [D loss: 0.695130, acc.: 45.31%] [G loss: 0.668823]\n",
            "598 [D loss: 0.691401, acc.: 50.00%] [G loss: 0.662563]\n",
            "599 [D loss: 0.691765, acc.: 48.44%] [G loss: 0.664512]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}